{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5183b9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b152d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 391\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "#(1)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the provided URL\n",
    "url = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv'\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "rows, columns = villagers_df.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(2)\n",
    "# Definition: Observations are the individual items of information.\n",
    "# Defintion: Variables are the types of information collected for each item in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6dd3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numeric columns:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "\n",
      "Value counts for 'species' column:\n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for 'personality' column:\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the URL\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# 1. Provide a summary of numeric columns using df.describe()\n",
    "numeric_summary = villagers_df.describe()\n",
    "\n",
    "# Display the numeric summary\n",
    "print(\"Summary statistics for numeric columns:\")\n",
    "print(numeric_summary)\n",
    "\n",
    "# 2. Use value_counts() for a categorical column (e.g., 'species')\n",
    "species_counts = villagers_df['species'].value_counts()\n",
    "\n",
    "# Display the value counts for the 'species' column\n",
    "print(\"\\nValue counts for 'species' column:\")\n",
    "print(species_counts)\n",
    "\n",
    "# You can use .value_counts() on any other categorical column as well, for example:\n",
    "personality_counts = villagers_df['personality'].value_counts()\n",
    "print(\"\\nValue counts for 'personality' column:\")\n",
    "print(personality_counts)\n",
    "\n",
    "\n",
    "# Loading the dataset: The pd.read_csv() method reads the CSV file from the given URL and stores it in the villagers_df DataFrame\n",
    "#df.describe(): This method summarizes the numeric columns in the dataset, showing statistics like count, mean, and standard deviation.\n",
    "#df['column'].value_counts(): This method counts the occurrences of each unique value in a categorical column like species or personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a497bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numeric columns:\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "\n",
      "Missing values in the dataset:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "Value counts for 'class' column:\n",
      "class\n",
      "Third     491\n",
      "First     216\n",
      "Second    184\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from the URL\n",
    "titanic_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(titanic_url)\n",
    "\n",
    "# 1. Provide a summary of numeric columns using df.describe()\n",
    "numeric_summary = titanic_df.describe()\n",
    "\n",
    "# Display the numeric summary\n",
    "print(\"Summary statistics for numeric columns:\")\n",
    "print(numeric_summary)\n",
    "\n",
    "# 2. Check for missing values in all columns\n",
    "missing_values = titanic_df.isnull().sum()\n",
    "\n",
    "# Display the missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(missing_values)\n",
    "\n",
    "# 3. Use value_counts() for a categorical column (e.g., 'class')\n",
    "class_counts = titanic_df['class'].value_counts()\n",
    "\n",
    "# Display the value counts for the 'class' column\n",
    "print(\"\\nValue counts for 'class' column:\")\n",
    "print(class_counts)\n",
    "\n",
    "#Load the dataset: The dataset is fetched from the URL and loaded into a pandas DataFrame.\n",
    "#df.describe(): Provides summary statistics for the numeric columns, including count, mean, min, max, etc.\n",
    "#Missing values: Counts the number of missing values in each column.\n",
    "#df['column'].value_counts(): Displays the counts of each unique value in the categorical class column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "\n",
    "# Attribute: A characteristic of an object that holds data. Accessed without parentheses. Example: df.shape (gives DataFrame dimensions).\n",
    "# Method: A function that performs an action on an object. Called with parentheses. Example: df.describe() (calculates summary statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6\n",
    "#Count: Tthe number of values that are not missing in the column.\n",
    "#Mean: The average of the values (sum of values divided by how many there are).\n",
    "#Standard Deviation (std): Shows how spread out the values are. A bigger number means more variation.\n",
    "#Min: The smallest value in the column.\n",
    "#25% (First Quartile): 25% of the values are below this number.\n",
    "#50% (Median): The middle value when the data is ordered. Half the values are above it, and half are below.\n",
    "#75% (Third Quartile): 75% of the values are below this number.\n",
    "#Max: The largest value in the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039bedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape before cleaning: (891, 14)\n",
      "Shape after cleaning: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "#(1)\n",
    "#A use case where df.dropna() might be preferred over del df['col'] is when only a small number of rows have missing values, and you want to preserve as much data as possible across all columns.\n",
    "#Example:\n",
    "#Suppose you have a dataset of customer transactions with the following columns: CustomerID, PurchaseAmount, and PaymentMethod. If only a few rows have missing values in the PaymentMethod column, you can use df.dropna() to remove those specific rows while keeping all columns intact.\n",
    "#using del df['col'] would delete the entire PaymentMethod column, which may not be ideal if only a few rows are missing values in that column.\n",
    "\n",
    "\n",
    "#(2)\n",
    "#If a column (e.g., Email) has many missing values (like 70%), it's better to delete the entire column with del df['Email']. This keeps all rows and other useful columns, avoiding losing too much data.\n",
    "#Using df.dropna() in this case would remove many rows, which might not be ideal if most of the data in other columns is complete.\n",
    "\n",
    "\n",
    "#(3)\n",
    "#Applying del df['col'] before df.dropna() is important because:\n",
    "#Deleting a column with many missing values first prevents unnecessary row loss. If you drop the problematic column, the remaining dataset is more complete.\n",
    "#If you run df.dropna() first, rows with missing values in that column will be removed, potentially discarding valuable data from other columns.\n",
    "#Example:\n",
    "#If a column like Email has many missing values, deleting it first ensures you retain more rows when you apply df.dropna() on the rest of the dataset. This helps preserve as much usable data as possible.\n",
    "\n",
    "#(4)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(titanic_url)\n",
    "\n",
    "# 1. Before cleaning: Check the number of missing values per column\n",
    "print(\"Before cleaning:\")\n",
    "print(titanic_df.isnull().sum())\n",
    "\n",
    "# 2. Delete the 'deck' column since it has too many missing values\n",
    "del titanic_df['deck']\n",
    "\n",
    "# 3. Drop rows with missing values in the remaining columns\n",
    "titanic_cleaned = titanic_df.dropna()\n",
    "\n",
    "# 4. After cleaning: Check the number of missing values per column\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(titanic_cleaned.isnull().sum())\n",
    "\n",
    "# Summary: Before and After\n",
    "print(\"\\nShape before cleaning:\", titanic_df.shape)\n",
    "print(\"Shape after cleaning:\", titanic_cleaned.shape)\n",
    "\n",
    "\n",
    "#Justification:\n",
    "#del df['deck']: The 'deck' column has too many missing values, so it's best to drop it entirely.\n",
    "#df.dropna(): This removes rows with missing values in other columns, ensuring the remaining dataset is complete.\n",
    "#Before and After:\n",
    "#Before: You will see a count of missing values in each column and the original shape of the dataset.\n",
    "#After: The missing values will be removed, and you’ll get the new shape of the dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66017f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "#(1)\n",
    "#Explanation:\n",
    "#df.groupby(\"col1\"): Groups the DataFrame (df) by the values in col1. This means that it creates separate subgroups of data for each unique value in col1.\n",
    "\n",
    "#[\"col2\"]: After grouping by col1, it selects the column col2 from these subgroups.\n",
    "\n",
    "#.describe(): For each group, it generates summary statistics (like count, mean, standard deviation, min, max, and quartiles) for the values in col2. The result is a descriptive summary for col2 within each group defined by col1.\n",
    "\n",
    "#Example using Titanic Dataset:\n",
    "#Let's say we want to group the Titanic data by the class of passengers and then summarize the statistics for their fare.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)\n",
    "\n",
    "# What this does:\n",
    "# Grouping by class: The data is split into three groups (First, Second, and Third class).\n",
    "# Describing fare: For each class group, it calculates the count, mean, standard deviation, and other summary statistics for the fare column.\n",
    "# Output Example (for fare grouped by class):\n",
    "#          count       mean        std  min    25%     50%     75%    max\n",
    "#class                                                                  \n",
    "#First      216  84.154687  78.380373  0.0  30.923  60.000  93.500  512.3292\n",
    "#Second     184  20.662183  13.417399  0.0  13.000  14.250  26.000   73.5000\n",
    "#Third      491  13.675550  11.778142  0.0   7.750   8.050  14.454   69.5500\n",
    "\n",
    "#Interpretation:\n",
    "#For First class passengers, there are 216 records, with an average fare of 84.15, and the fare ranges from 0 to 512.33.\n",
    "#For Third class passengers, the mean fare is much lower, at 13.68, with a max fare of 69.55.\n",
    "#Summary:\n",
    "#groupby(\"col1\")[\"col2\"].describe() groups the data by col1 and provides summary statistics for col2 in each group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca861e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2)\n",
    "#The count function works differently in describe() and groupby():\n",
    "#In describe(), count tells you how many non-missing values are in the entire dataset.\n",
    "#In groupby(), count shows how many non-missing values are in each group, helping you see how data is spread across categories like \"col1\" or \"col2\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c31434c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#(3)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#(a)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n\u001b[1;32m      9\u001b[0m grouped_summary \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#(3)\n",
    "#(a)\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ecaebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtitanics\u001b[49m\u001b[38;5;241m.\u001b[39mcsv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanics' is not defined"
     ]
    }
   ],
   "source": [
    "#(b)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanics.csv = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056e911b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m grouped_summary \u001b[38;5;241m=\u001b[39m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Display the summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "#(c)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = DF.groupby(\"col1\")[\"col2\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c256f310",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (1795687101.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(url\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#(d)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = df.groupby(\"class\")[\"fare\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b7b8a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'col1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m grouped_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescrible()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Display the summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_summary)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'col1'"
     ]
    }
   ],
   "source": [
    "#(e)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = df.groupby(\"col1\")[\"col2\"].describle()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b13aff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m grouped_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Display the summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic_df' is not defined"
     ]
    }
   ],
   "source": [
    "#(f)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = titanic_df.groupby(\"sex\")[\"age\"].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411e4996",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titanic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Group by 'class' and describe the 'fare' for each class\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m grouped_summary \u001b[38;5;241m=\u001b[39m \u001b[43mtitanic_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m)[age]\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Display the summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_summary)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic_df' is not defined"
     ]
    }
   ],
   "source": [
    "#(g)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'class' and describe the 'fare' for each class\n",
    "grouped_summary = titanic_df.groupby(\"sex\")[age].describe()\n",
    "\n",
    "# Display the summary\n",
    "print(grouped_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a47fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n",
    "#Mostly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317cb53",
   "metadata": {},
   "source": [
    "To summarize a chatbot session, a concluding prompt like the following could be used to efficiently capture the key exchanges:\n",
    "\n",
    "Example Concluding Prompt:\n",
    "\"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment.\"\n",
    "\n",
    "This request would lead to a concise recap of all the important points discussed. For example, in this session:\n",
    "\n",
    "Summary of Our Exchanges:\n",
    "Data Operations: We discussed how to use pandas to load datasets, such as the Titanic dataset, from URLs and explored operations like grouping and summarizing data using methods like df.describe() and groupby().\n",
    "\n",
    "Error Fixing: We corrected errors in the code, particularly regarding dataset URLs and the use of column names like class, which is a reserved keyword in Python.\n",
    "\n",
    "Handling Missing Data: We covered the difference between df.dropna() and del df['col'], with examples of when to use each for efficiently managing missing values.\n",
    "\n",
    "Summary Statistics: Definitions of statistics like count, mean, and standard deviation provided by df.describe() were explained in simple terms.\n",
    "\n",
    "This concise recap captures the essential details of our interactions that could be submitted for review.\n",
    "\n",
    "https://chatgpt.com/share/66e3576c-6d90-800a-bf30-3aaede23c88f\n",
    "\n",
    "\n",
    "Summary of Our Interactions:\n",
    "\n",
    "Dataset Download and Analysis:\n",
    "\n",
    "You mentioned downloading a dataset about characters from Animal Crossing from a given URL.\n",
    "I helped guide you through using Python with pandas to load the dataset and extract the number of rows and columns.\n",
    "Definitions of Observations and Variables:\n",
    "\n",
    "You asked for the general definitions of \"observations\" and \"variables.\"\n",
    "I provided simplified explanations:\n",
    "Observations: The individual items or data points in a dataset (typically rows).\n",
    "Variables: The characteristics or types of information collected for each observation (typically columns).\n",
    "\n",
    "Summarizing Dataset Columns:\n",
    "\n",
    "I provided Python code to help summarize the dataset’s columns using pandas, including methods for summarizing numerical and categorical data.\n",
    "\n",
    "https://chatgpt.com/share/66e3577f-db30-800a-a47e-556b5029b4d6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
